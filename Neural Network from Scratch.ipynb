{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Neural Network from scratch\n",
    "\n",
    "1. Data sample( independent variables and dependent variables)\n",
    "2. Define Hyper parameters\n",
    "3. Define ACtivation function and the derivative\n",
    "4. Train the model\n",
    "4. Make predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building model\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://i.imgur.com/5EHSfO6.png\" alt='cat-noncat' width='75%' />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The general methodology to build a Neural Network is to:\n",
    "\n",
    "1. Define the neural network structure(- input layer - hidden layer -output layer)\n",
    "2. Initialize the model's parameters\n",
    "3. Loop\n",
    "- Implement forward propagation\n",
    "- Copute Loss\n",
    "- Implement back ward probagation to get the gradients\n",
    "- Update parameters( gradient descent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "1. INput Layer: X (row) each object\n",
    "2. Initialize Weights and Bias: W1, B1\n",
    "3. $Z^{[1]} = W^{[1]}X^T + b^{[1]}$\n",
    "4. $A^{[1]} = relu(Z^{[1]})$\n",
    "5. $Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$\n",
    "6. $A^{[2]} = sigmoid(Z^{[2]}) = \\hat{y}$\n",
    "\n",
    "\n",
    "- Loss function/ Cost Function/ Objective Function\n",
    "\n",
    "- $J = y - \\hat{y} = error$\n",
    "- $J = -\\frac1m\\sum \\bigg(Y \\odot log(A^{[2]}) + (1-Y) \\odot log(1-A^{[2]})\n",
    "\\bigg)$\n",
    "\n",
    "> Note that $\\odot$ denotes element wise multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation\n",
    "1. $dZ{[2]} = (A^{[2]} - Y)$\n",
    "2. $dW^{[2]} = \\frac{1}{m} dZ^{[2]}A^{[1]^T}$\n",
    "3. $db^{[2]} = \\frac{1}{m}np.sum(dZ^{[2]}, axis=1, keepdims=True)$\n",
    "4. $dZ^{[1]} =   W^{[2]^T}dZ^{[2]} \\odot g^{[1]'}(Z^{[1]})$\n",
    "5. $dW^{[1]} = \\frac1mdZ^{[1]}X$\n",
    "6. $db^{[1]} = \\frac{1}{m}np.sum(dZ^{[1]}, axis=1, keepdims=True)$\n",
    "\n",
    "> Note that $\\odot$ denotes elementwise multiplication.\n",
    "\n",
    "> The notation you will use is common in deep learning coding:\n",
    ">    - dW1 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_1 }$\n",
    ">    - db1 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_1 }$\n",
    ">    - dW2 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_2 }$\n",
    ">    - db2 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_2 }$\n",
    "\n",
    "> To compute dZ1 you'll need to compute $g^{[1]'}(Z^{[1]})$. Since $g^{[1]}(.)$ is the tanh activation function, if $a = g^{[1]}(z)$ then $g^{[1]'}(z) = 1-a^2$. So you can compute \n",
    "    $g^{[1]'}(Z^{[1]})$ using `(1 - A1**2)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network():\n",
    "    def __init__(self, n_h, learning_rate, n_iterations):\n",
    "        self.n_h = n_h\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        \n",
    "    def relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def relu_backward(self, Z):\n",
    "        result = np.ones(Z.shape)\n",
    "        result[Z < 0] = 0\n",
    "        return result\n",
    "\n",
    "    def layer_sizes(self, X, Y, n_h):\n",
    "        n_x = X.shape[1]\n",
    "        n_h = n_h\n",
    "        n_y = Y.shape[0]\n",
    "        \n",
    "    def initialize_params(self, n_x, n_h, n_y):\n",
    "        np.random.seed(102)\n",
    "        W1 = np.random.randn(n_h, n_x) * np.sqrt(1/n_x)\n",
    "        b1 = np.zeros((n_h, 1))\n",
    "        W2 = np.random.randn(n_y, n_h) * np.sqrt(1/n_h)\n",
    "        b2 = np.zeros((n_y, 1))\n",
    "        \n",
    "        assert (W1.shape==(n_h, n_x))\n",
    "        assert (W2.shape==(n_y, n_h))\n",
    "        assert (b1.shape==(n_h, 1))\n",
    "        assert (b2.shape==(n_y, 1))\n",
    "        \n",
    "        self.parameters = {\n",
    "            'W1': W1,\n",
    "            'W2': W2,\n",
    "            'b1': b1,\n",
    "            'b2': b2\n",
    "        }\n",
    "        \n",
    "    def forward(self, X, parameters):\n",
    "        W1 = self.parameters['W1']\n",
    "        W2 = self.parameters['W2']\n",
    "        b1 = self.parameters['b1']\n",
    "        b2 = self.parameters['b2']\n",
    "\n",
    "        Z1 = np.dot(W1, X.T) + b1\n",
    "        A1 = relu(Z1)\n",
    "        Z2 = np.dot(W2, A1) + b2\n",
    "        A2 = sigmoid(Z2)\n",
    "\n",
    "        return (Z1, A1, Z2, A2)\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        Z1, A1, Z2, A2 = self.forward(X, self.parameters)\n",
    "        \n",
    "        self.cache = {\n",
    "            'Z1': Z1,\n",
    "            'A1': A1,\n",
    "            'Z2': Z2,\n",
    "            'A2': A2\n",
    "        }\n",
    "        return A2\n",
    "    def compute_cross_entropy_cost(self, A2, Y):\n",
    "        m = Y.shape[1]\n",
    "        loss = np.multiply(Y, np.log(A2)) + np.multiply(1-Y, np.log(1-A2))\n",
    "        cost = -(1/m) * np.sum(loss)\n",
    "        return cost\n",
    "    \n",
    "    def backward_propagation(self, X, Y):\n",
    "        W1 = self.parameters['W1']\n",
    "        W2 = self.parameters['W2']\n",
    "        b1 = self.parameters['b1']\n",
    "        b2 = self.parameters['b2']\n",
    "\n",
    "        A1 = self.cache['A1']\n",
    "        A2 = self.cache['A2']\n",
    "        Z1 = self.cache['Z1']\n",
    "        Z2 = self.cache['Z2']\n",
    "\n",
    "        m = Y.shape[1]\n",
    "\n",
    "        dZ2 = A2 - Y\n",
    "        dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        dZ1 = np.multiply(np.dot(W2.T, dZ2), relu_backward(Z1))\n",
    "        dW1 = (1/m)*np.dot(dZ1, X)\n",
    "        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "        \n",
    "        self.grads = {\n",
    "            'dW1': dW1,\n",
    "            'dW2': dW2,\n",
    "            'db1': db1,\n",
    "            'db2': db2\n",
    "        }\n",
    "        return self.grads\n",
    "    \n",
    "    def update_parameters(self):\n",
    "        W1 = self.parameters['W1']\n",
    "        W2 = self.parameters['W2']\n",
    "        b1 = self.parameters['b1']\n",
    "        b2 = self.parameters['b2']\n",
    "\n",
    "        dW1 = self.grads['dW1']\n",
    "        dW2 = self.grads['dW2']\n",
    "        db1 = self.grads['db1']\n",
    "        db2 = self.grads['db2']\n",
    "\n",
    "        W1 = W1 - self.learning_rate * dW1\n",
    "        W2 = W2 - self.learning_rate * dW2\n",
    "        b1 = b1 - self.learning_rate * db1\n",
    "        b2 = b2 - self.learning_rate * db2\n",
    "        \n",
    "        self.parameters = {\n",
    "            'W1': W1,\n",
    "            'W2': W2,\n",
    "            'b1': b1,\n",
    "            'b2': b2\n",
    "        }\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        n_x, n_h, n_y = self.layer_sizes(X, Y, self.n_h)\n",
    "\n",
    "        self.initialize_params(n_x, n_h, n_y)\n",
    "\n",
    "        costs = []\n",
    "        test_errors = []\n",
    "        for i in range(self.n_iterations):\n",
    "            # Forward propagation\n",
    "            A2 = self.forward_propagation(X)\n",
    "            cost = self.compute_cross_entropy_cost(A2, Y)\n",
    "            costs.append(cost)\n",
    "            # Backward propagation\n",
    "            self.backward_propagation(X, Y)\n",
    "            # Update parameters\n",
    "            self.update_parameters()\n",
    "\n",
    "            if (i % 100)==0:\n",
    "                print(f'Iteration {i}, Cost: {cost}')\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 8))\n",
    "        plt.plot(costs)\n",
    "        plt.plot(test_errors)\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('iterations')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        _, _, _, A2 = self.forward(X, self.parameters)\n",
    "        predictions = A2 > 0.5\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Neural_Network(7, 0.001, 500)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy Score : %f\" % accuracy_score(y_test[0], predictions[0]))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test[0], predictions[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1. / (1 + np.exp(-Z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(Z):\n",
    "    result = np.ones(Z.shape)\n",
    "    result[Z < 0] = 0\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y, hidden_neuron):\n",
    "    input_neuron = X.shape[1]\n",
    "    hidden_neuron = hidden_neuron\n",
    "    output_neuron = Y.shape[0]\n",
    "    \n",
    "    return(input_neuron, hidden_neuron, output_neuron)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(in_n, hi_n, ou_n):\n",
    "    np.random.seed(102)\n",
    "    W1 = np.random.randn(hi_n, in_n) * np.sqrt(1/in_n)\n",
    "    b1 = np.zeros((hi_n, 1))\n",
    "    W2 = np.random.randn(ou_n, hi_n) * np.sqrt(1/hi_n)\n",
    "    b2 = np.zeros((ou_n, 1))\n",
    "    \n",
    "    assert (W1.shape == (hi_n, in_n))\n",
    "    assert (W2.shape == (ou_n, hi_n))\n",
    "    assert (b1.shape == (hi_n, 1))\n",
    "    assert (b2.shape == (ou_n, 1))\n",
    "    \n",
    "    parameters = {\n",
    "        'W1': W1,\n",
    "        'W2': W2,\n",
    "        'b1': b1,\n",
    "        'b2': b2\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    b1 = parameters['b1']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    Z1 = np.dot(W1, X.T) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) +b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    \n",
    "    cache = {\n",
    "        'Z1': Z1,\n",
    "        'A1': A1,\n",
    "        'Z2': Z2,\n",
    "        'A2': A2\n",
    "    }\n",
    "    return A2, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_entropy(A2, Y):\n",
    "    m = Y.shape[1]\n",
    "    loss = np.multiply(Y, np.log(A2)) + np.multiply(1-Y, np.log(1-A2))\n",
    "    cost = -(1/m) * np.sum(loss)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, Y, parameters, cache):\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    b1 = parameters['b1']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    Z1 = cache['Z1']\n",
    "    Z2 = cache['Z2']\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), relu_backward(Z1))\n",
    "    dW1 = (1/m) * np.dot(dZ1, X)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {\n",
    "        'dW1': dW1,\n",
    "        'dW2': dW2,\n",
    "        'db1': db1,\n",
    "        'db2': db2\n",
    "    }\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    b1 = parameters['b1']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    dW1 = grads['dW1']\n",
    "    dW2 = grads['dW2']\n",
    "    db1 = grads['db1']\n",
    "    db2 = grads['db2']\n",
    "    \n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    parameters = {\n",
    "        'W1': W1,\n",
    "        'W2': W2,\n",
    "        'b1': b1,\n",
    "        'b2': b2\n",
    "    }\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, hi_n, iterations, learning_rate, parameters=None):\n",
    "    in_n, hi_n, ou_n = layer_sizes(X, Y, hi_n)\n",
    "    \n",
    "    if parameters is None:\n",
    "        parameters = initialize_params(in_n, hi_n, ou_n)\n",
    "        \n",
    "    costs = []\n",
    "    test_errors = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        #Forward propagation\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        cost = compute_cross_entropy(A2, Y)\n",
    "        costs.append(cost)\n",
    "        \n",
    "        # Backward propagation\n",
    "        grads = backward_propagation(X, Y, parameters, cache)\n",
    "        \n",
    "        #update parameters\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Test error\n",
    "        A2_test, _ = forward_propagation(X_test, parameters)\n",
    "        test_errors.append(compute_cross_entropy(A2_test, y_test))\n",
    "        \n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}, Cost: {cost}\")\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 8))\n",
    "    \n",
    "    plt.plot(costs)\n",
    "    plt.plot(test_errors)\n",
    "    plt.ylabel('Cost')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    \n",
    "    predictions = A2 > 0.5\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = nn_model(X_train, y_train, hi_n=16, iterations=500, \n",
    "#                       learning_rate=0.0001)\n",
    "# predictions = predict(X_test, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data independent variable (x)\n",
    "\n",
    "training_set = np.array([[0, 1, 0], [0, 0, 1],\n",
    "                         [1, 0, 0], [1, 1, 0],\n",
    "                         [1, 1, 1], [0, 1, 1],\n",
    "                         [0, 1, 0]])\n",
    "# 3 featrues, 7 entries - objects\n",
    "\n",
    "#training data dependent variables (y)\n",
    "labels = np.array([[1, 0, 0, 1, 1, 0, 1]])\n",
    "\n",
    "#reshape dependent variable\n",
    "\n",
    "labels = labels.reshape(7, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyper parameters\n",
    "\n",
    "np.random.seed(42)\n",
    "weights = np.random.rand(3, 1)\n",
    "bias = np.random.rand(1)\n",
    "learning_rate = 0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37454012],\n",
       "       [0.95071431],\n",
       "       [0.73199394]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59865848]\n"
     ]
    }
   ],
   "source": [
    "print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Function\n",
    "# methods\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "for epoch in range(30000):\n",
    "    inputs = training_set\n",
    "    XW = np.dot(inputs, weights) * bias\n",
    "    z = sigmoid(XW)\n",
    "    error = z - labels\n",
    "    print(error.sum())\n",
    "    dcost = error\n",
    "    dpred = sigmoid_derivative(z)\n",
    "    z_del = dcost * dpred\n",
    "    inputs = training_set.T\n",
    "    weights = weigths - learning_rate * np.dot(inputs, z_del)\n",
    "    for num in z_del:\n",
    "        bias = bias = learning_rate * num\n",
    "        \n",
    "inputs = training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODel feed-forward nerural nerwork\n",
    "\n",
    "# Feed Forward\n",
    "XW = np.dot(inputs, weights * bias)\n",
    "z = sigmoid(XW)\n",
    "\n",
    "#error\n",
    "error = z - labels\n",
    "print(error.sum())\n",
    "\n",
    "# Determining slope\n",
    "slope = inputs * dcost * dpred\n",
    "\n",
    "dcost = error\n",
    "dpred = sigmoid_derivative(z)\n",
    "z_del = dcost * dpred\n",
    "inputs = training_set.T\n",
    "weights = weights = learning_rate * np.dot(inputs, z_del)\n",
    "\n",
    "for sum in z_del:\n",
    "    bias = bias - learning_rate * num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting outcomes\n",
    "\n",
    "single_pt = np.array([1, 0, 0])\n",
    "result = sigmoid(np.dot(single_pt, weights) + bias)\n",
    "\n",
    "print(result)\n",
    "\n",
    "single_pt = np.array([0, 1, 0])\n",
    "\n",
    "result = sigmoid(no.dot(single_pt, weights) + bias)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
