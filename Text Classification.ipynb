{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import textblob\n",
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import xgboost\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'Costa Rican Household Poverty Level Prediction.ipynb',\n",
       " 'Data Scientist Tutorial.ipynb',\n",
       " 'Dataset',\n",
       " 'Explanation of Embedding Layer.ipynb',\n",
       " 'Home Credit Default Risk.ipynb',\n",
       " 'House Prices Predict.ipynb',\n",
       " 'Quora question Pairs.ipynb',\n",
       " 'Recommendation System.ipynb',\n",
       " 'SQL.ipynb',\n",
       " 'Text Classification.ipynb',\n",
       " 'Text data.ipynb',\n",
       " 'Text Preprocessing.ipynb',\n",
       " 'Token Text Data.ipynb',\n",
       " 'Toxic Comment Classification Challenge.ipynb',\n",
       " 'Twitter Sentiment Analysis.ipynb']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get location\n",
    "os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "data = open('Dataset/corpus.txt', encoding='utf-8' ).read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split('\\n')):\n",
    "    sentence = line.split()\n",
    "    labels.append(sentence[0])\n",
    "    texts.append(\" \".join(sentence[1:]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       label\n",
       "0  Stuning even for the non-gamer: This sound tra...  __label__2\n",
       "1  The best soundtrack ever to anything.: I'm rea...  __label__2\n",
       "2  Amazing!: This soundtrack is my favorite music...  __label__2\n",
       "3  Excellent Soundtrack: I truly like this soundt...  __label__2\n",
       "4  Remember, Pull Your Jaw Off The Floor After He...  __label__2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe using texts labels\n",
    "train = pd.DataFrame()\n",
    "train['text'] = texts\n",
    "train['label'] = labels\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>A revelation of life in small town America in ...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Great biography of a very interesting journali...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Interesting Subject; Poor Presentation: You'd ...</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Don't buy: The box looked used and it is obvio...</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Beautiful Pen and Fast Delivery.: The pen was ...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text       label\n",
       "0     Stuning even for the non-gamer: This sound tra...  __label__2\n",
       "1     The best soundtrack ever to anything.: I'm rea...  __label__2\n",
       "2     Amazing!: This soundtrack is my favorite music...  __label__2\n",
       "3     Excellent Soundtrack: I truly like this soundt...  __label__2\n",
       "4     Remember, Pull Your Jaw Off The Floor After He...  __label__2\n",
       "...                                                 ...         ...\n",
       "9995  A revelation of life in small town America in ...  __label__2\n",
       "9996  Great biography of a very interesting journali...  __label__2\n",
       "9997  Interesting Subject; Poor Presentation: You'd ...  __label__1\n",
       "9998  Don't buy: The box looked used and it is obvio...  __label__1\n",
       "9999  Beautiful Pen and Fast Delivery.: The pen was ...  __label__2\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'text': texts,\n",
    "     'label': labels}\n",
    "data1 = pd.DataFrame(d)\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       label\n",
       "0  Stuning even for the non-gamer: This sound tra...  __label__2\n",
       "1  The best soundtrack ever to anything.: I'm rea...  __label__2\n",
       "2  Amazing!: This soundtrack is my favorite music...  __label__2\n",
       "3  Excellent Soundtrack: I truly like this soundt...  __label__2\n",
       "4  Remember, Pull Your Jaw Off The Floor After He...  __label__2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split and shuffle the dataset into trainig and validation dataset\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train['text'],\n",
    "                                                      train['label'], random_state=102)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7500,), (2500,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['__label__1', '__label__2'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode the target variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit_transform(train['label'])\n",
    "\n",
    "train_y = encoder.transform(train_y)\n",
    "valid_y = encoder.transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "- Raw text data will be transformed into feature vectors\n",
    "\n",
    "\n",
    "1. Bag of words - COunt vectors as features\n",
    "2. TFIDF = Vectors as features\n",
    "- Word level\n",
    "- N-Gram level\n",
    "- Character level\n",
    "3. Word embedding as features\n",
    "4. Text/NLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset\n",
    "- label\n",
    "- text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='\\\\w{1,}', tokenizer=None,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Create a count vectorizer object\n",
    "\n",
    "count = CountVectorizer(analyzer='word', token_pattern=r\"\\w{1,}\")\n",
    "count.fit(train['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform teh training and validation data using count vectorizer object\n",
    "\n",
    "train_count = count.transform(train_x)\n",
    "valid_count = count.transform(valid_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.get_feature_names()\n",
    "train_count.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = train_count.toarray()\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7500x31666 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 437119 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2. TF-IDF Vectors as features\n",
    "# - word level\n",
    "# - N-Gram level\n",
    "# - Character level\n",
    "\n",
    "# word level\n",
    "tfidf = TfidfVectorizer(analyzer='word', token_pattern=r\"\\w{1,}\", max_features=5000)\n",
    "tfidf.fit(train['text'])\n",
    "\n",
    "train_tfidf = tfidf.transform(train_x)\n",
    "valid_tfidf = tfidf.transform(valid_x)\n",
    "\n",
    "\n",
    "# n-gram level\n",
    "\n",
    "tfidf_ngram = TfidfVectorizer(analyzer='word', token_pattern=r\"w{1,}\",\n",
    "                              ngram_range=(2, 3), max_features=5000)\n",
    "tfidf_ngram.fit(train['text'])\n",
    "\n",
    "train_tfidf_ngram = tfidf_ngram.transform(train_x)\n",
    "valid_tfidf_ngram = tfidf_ngram.transform(valid_x)\n",
    "\n",
    "\n",
    "# character level\n",
    "\n",
    "tfidf_chars = TfidfVectorizer(analyzer='char', token_pattern=r\"\\w{1,}\",\n",
    "                              ngram_range=(2, 3), max_features=5000)\n",
    "tfidf_chars.fit(train['text'])\n",
    "\n",
    "train_tfidf_ngram = tfidf_chars.transform(train_x)\n",
    "valid_tfidf_ngram = tfidf_chars.transform(valid_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "- A Word Embeddings i a from of representing words and documents using a dense vectoer representation.\n",
    "-  The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used\n",
    "- Word embeddings can be trained using the input corpus itself \n",
    "- Or can be generated using pre trained word embeddings such as Glove, FastText, Word2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use pre- trained word embeddings in the model\n",
    "\n",
    "1. Loading the pre trained word embeddings\n",
    "2. Create a tokenizer object\n",
    "3. Transforming text documents to sequence of tokens and pad them\n",
    "4. Create a mapping of token and their respective embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the pre-trained word-embedding vectors \n",
    "# embeddings_index = {}\n",
    "# for i, line in enumerate(open('data/wiki-news-300d-1M.vec')):\n",
    "#     values = line.split()\n",
    "#     embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# # create a tokenizer \n",
    "# token = text.Tokenizer()\n",
    "# token.fit_on_texts(trainDF['text'])\n",
    "# word_index = token.word_index\n",
    "\n",
    "# # convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "# train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "# valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# # create token-embedding mapping\n",
    "# embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "# for word, i in word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text based Features\n",
    "1. Word Count of documents - total number of words in the documents\n",
    "2. Character count of the documents\n",
    "3. Average Word Density of the ducoment - average length of the words used in the documents\n",
    "4. Punctuation COunt in the conplete essay\n",
    "5. Upper case count in the complete essay\n",
    "6. Title word Count in the complete essay\n",
    "7. Frequency distribution of Part of Speech Tags\n",
    "- NOuns, Verb, Adj, Adv, Pronoun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>A revelation of life in small town America in ...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Great biography of a very interesting journali...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Interesting Subject; Poor Presentation: You'd ...</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Don't buy: The box looked used and it is obvio...</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Beautiful Pen and Fast Delivery.: The pen was ...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text       label\n",
       "0     Stuning even for the non-gamer: This sound tra...  __label__2\n",
       "1     The best soundtrack ever to anything.: I'm rea...  __label__2\n",
       "2     Amazing!: This soundtrack is my favorite music...  __label__2\n",
       "3     Excellent Soundtrack: I truly like this soundt...  __label__2\n",
       "4     Remember, Pull Your Jaw Off The Floor After He...  __label__2\n",
       "...                                                 ...         ...\n",
       "9995  A revelation of life in small town America in ...  __label__2\n",
       "9996  Great biography of a very interesting journali...  __label__2\n",
       "9997  Interesting Subject; Poor Presentation: You'd ...  __label__1\n",
       "9998  Don't buy: The box looked used and it is obvio...  __label__1\n",
       "9999  Beautiful Pen and Fast Delivery.: The pen was ...  __label__2\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['char_count'] = train['text'].apply(len)\n",
    "train['word_count'] = train['text'].apply(lambda x: len(x.split()))\n",
    "train['word_density'] = train['char_count'] / (train['word_count'] + 1)\n",
    "train['punctuation_count'] = train['text'].apply(lambda x: len(''.join(_ for _ in x if _ in string.punctuation)))\n",
    "train['title_word_count'] = train['text'].apply(lambda x: len([word for word in x.split() if word.istitle()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_family = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "def check_pos_tag(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt\n",
    "\n",
    "train['noun_count'] = train['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "train['verb_count'] = train['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "train['adj_count'] = train['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "train['adv_count'] = train['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "train['pron_count'] = train['text'].apply(lambda x: check_pos_tag(x, 'pron'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_char</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>pron_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>426</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>5.259259</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>509</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>5.193878</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>760</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>5.846154</td>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "      <td>39</td>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>743</td>\n",
       "      <td>118</td>\n",
       "      <td>118</td>\n",
       "      <td>6.243697</td>\n",
       "      <td>33</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>481</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>5.465909</td>\n",
       "      <td>22</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       label  char_count  \\\n",
       "0  Stuning even for the non-gamer: This sound tra...  __label__2         426   \n",
       "1  The best soundtrack ever to anything.: I'm rea...  __label__2         509   \n",
       "2  Amazing!: This soundtrack is my favorite music...  __label__2         760   \n",
       "3  Excellent Soundtrack: I truly like this soundt...  __label__2         743   \n",
       "4  Remember, Pull Your Jaw Off The Floor After He...  __label__2         481   \n",
       "\n",
       "   word_char  word_count  word_density  punctuation_count  title_word_count  \\\n",
       "0         80          80      5.259259                 11                10   \n",
       "1         97          97      5.193878                 14                 7   \n",
       "2        129         129      5.846154                 40                24   \n",
       "3        118         118      6.243697                 33                52   \n",
       "4         87          87      5.465909                 22                30   \n",
       "\n",
       "   noun_count  verb_count  adj_count  adv_count  pron_count  \n",
       "0          20          15          6          6          11  \n",
       "1          20          23          9          3          10  \n",
       "2          39          18         13         10          11  \n",
       "3          52          12          9          2           7  \n",
       "4          31          13          7          2           9  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Stuning even for the non-gamer: This sound tra...\n",
       "1       The best soundtrack ever to anything.: I'm rea...\n",
       "2       Amazing!: This soundtrack is my favorite music...\n",
       "3       Excellent Soundtrack: I truly like this soundt...\n",
       "4       Remember, Pull Your Jaw Off The Floor After He...\n",
       "                              ...                        \n",
       "9995    A revelation of life in small town America in ...\n",
       "9996    Great biography of a very interesting journali...\n",
       "9997    Interesting Subject; Poor Presentation: You'd ...\n",
       "9998    Don't buy: The box looked used and it is obvio...\n",
       "9999    Beautiful Pen and Fast Delivery.: The pen was ...\n",
       "Name: text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Models as Features\n",
    "- Topic modelling is a technique to identify the groups of words from a collection of documents that contains best information in the collection\n",
    "\n",
    "-  LDA is an iterative model which starts from a fixed number of topics. Each topic is represented as a distribution over words, and each document is then represented as a distribution over topics. \n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a Latent Dirichlet Allocation\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online',\n",
    "                                                   max_iter=20)\n",
    "topics = lda_model.fit_transform(train_count)\n",
    "\n",
    "topic_word = lda_model.components_\n",
    "vocab = count.get_feature_names()\n",
    "\n",
    "# view the topic models\n",
    "\n",
    "n_top_words = 10\n",
    "\n",
    "topic_summaries = []\n",
    "for i, topic_dis in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dis)][:-(n_top_words+1): -1]\n",
    "    topic_summaries.append(' '.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['voodoo scooter napoleon ulysses wished bela nbc dover differential writting',\n",
       " 'female male henry italian successful rod gorgeous mrs interpretation ya',\n",
       " 'rice thoroughly rabbit beads freud baseball cooker bore porn absurd',\n",
       " 'the and a of to i this is it book',\n",
       " 'the is of this a and cd s music album',\n",
       " 'bands count build emarker blocks spending hurry chest rain juvenile',\n",
       " 'cartridge theology sticks flint clippers wagner nb entitled recalled cowboy',\n",
       " 'boots boot thus hiking damage waterproof printed police socks gluten',\n",
       " 'african insights wars village bland buendia solitude recall rise devoid',\n",
       " 'la de en y con el los sin que un',\n",
       " 'relationship double grammar essential editor management snmp prehistoric enter merely',\n",
       " 'countries fence germany systems map rigid tile criticize samsung cream',\n",
       " 'recordings nirvana chase venus wit professor stewart mentions arab orchestra',\n",
       " 'the to it is you and a for game this',\n",
       " 'costume daily dishes pratchett waist cincher discworld pregnancy wedding eating',\n",
       " 'secret west killer central musiq cornwell scarpetta joe strategy layout',\n",
       " 'the i it and to a this for of not',\n",
       " 'walking photography l miles initial angry jennifer location weather connecting',\n",
       " 'haiku heater inspiring disney messages retail 4th warming greece redgrave',\n",
       " 'et swing le pour bargain est lab pas viking des']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model \n",
    "#### Text Classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid,\n",
    "                is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    # Predict the labels on the validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    return metrics.accuracy_score(predictions, valid_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NB, Count Vectors: 0.8428\n",
      "NB, Wordlevel TF-IDF: 0.8492\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "# on Count Vectors\n",
    "\n",
    "naive_accuracy = train_model(naive_bayes.MultinomialNB(), train_count, train_y,\n",
    "                             valid_count)\n",
    "print(f\" NB, Count Vectors: { naive_accuracy}\")\n",
    "\n",
    "# Naive Bayes on word level TFIDF vectors\n",
    "\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), train_tfidf, train_y,\n",
    "                       valid_tfidf)\n",
    "print( f\"NB, Wordlevel TF-IDF: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors: 0.8592\n",
      "LR, Wordlevel TF-IDF: 0.8696\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier\n",
    "accuracy = train_model(linear_model.LogisticRegression(), train_count,\n",
    "                       train_y, valid_count)\n",
    "\n",
    "print(f\"LR, Count Vectors: {accuracy}\")\n",
    "\n",
    "# Linear Classifier on word level TF-IDF Vectors\n",
    "\n",
    "accuracy = train_model(linear_model.LogisticRegression(),\n",
    "                       train_tfidf, train_y, valid_tfidf)\n",
    "print(f\"LR, Wordlevel TF-IDF: { accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HT\\Miniconda3\\envs\\cs_ftmle\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, WordLevel vectors: 0.4964\n"
     ]
    }
   ],
   "source": [
    "# SVM Model\n",
    "\n",
    "accuracy = train_model(svm.SVC(), train_tfidf, train_y, valid_tfidf)\n",
    "print(f\"SVM, WordLevel vectors: { accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HT\\Miniconda3\\envs\\cs_ftmle\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors: 0.7364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HT\\Miniconda3\\envs\\cs_ftmle\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, WordLevel TF-IDF: 0.7596\n"
     ]
    }
   ],
   "source": [
    "# Bagging model\n",
    "\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(),\n",
    "                       train_count, train_y, valid_count)\n",
    "\n",
    "print(f\"RF, Count Vectors: { accuracy}\")\n",
    "\n",
    "# Random Forest on word level TF -IDF vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), train_tfidf,\n",
    "                       train_y, valid_tfidf)\n",
    "print(f\"RF, WordLevel TF-IDF: { accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting Model\n",
    "# Boosting mdoels are another type of ensemble models part of tree based models\n",
    "# Boosting is a machine learning ensemble meta-algorithm for primarily reducing bias, \n",
    "#and also variance in supervised learning, and a family of machine learning algorithms \n",
    "#that convert weak learners to strong ones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors: 0.7968\n",
      "XGB, WordLevel TF-IDF: 0.7976\n"
     ]
    }
   ],
   "source": [
    "#Extreme Gradient Boosting on Count Vectors\n",
    "\n",
    "accuracy = train_model(xgboost.XGBClassifier(), train_count.tocsc(),\n",
    "                       train_y, valid_count.tocsc())\n",
    "print(f\"Xgb, Count Vectors: { accuracy}\")\n",
    "\n",
    "# XGB model on word Level TFIDF vectors\n",
    "\n",
    "accuracy = train_model(xgboost.XGBClassifier(), train_tfidf.tocsc(),\n",
    "                       train_y, valid_tfidf.tocsc())\n",
    "\n",
    "print(f\"XGB, WordLevel TF-IDF: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shallow model\n",
    "# Neural network contains mainly 3 typers of layers\n",
    "# inputlayer, hiddenlayer, out layer\n",
    "\n",
    "def create_model(input_size):\n",
    "    #  create input layer\n",
    "    input_layer = layers.Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    \n",
    "    hidden_layer = layers.Dense(100, activation='relu')(input_layer)\n",
    "    # create outlayer\n",
    "    \n",
    "    output_layer = layers.Dense(1, activation='sigmoid')(hidden_layer)\n",
    "    \n",
    "    classifier = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss=\"binary_crossentropy\")\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 5000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 5000)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf_ngram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7500x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3381727 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = create_model(train_tfidf_ngram.shape[1])\n",
    "# accuracy = train_model(classifier, train_tfidf_ngram, train_y, valid_tfidf_ngram,\n",
    "#                        is_neural_net=True)\n",
    "\n",
    "# print(f\"NN, WordLevel TF-IDF Vectors: { accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep neural network\n",
    "# Convolutional Neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer\n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(train['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    3,    13,    29,   223,    75,     1,   317,     9,    33,\n",
       "           3,    98,   747,    18,     1,    77,   243,     7,     9,\n",
       "         173,     5,   163,   327,    75,     1,  3770,    23,    26,\n",
       "        1055,    15,    21,     5,    21,   917,     6, 20901,     5,\n",
       "          52,     7,   229,    55,   356,    61,    79,     8,    80,\n",
       "          23,   181,     5,   412,    17,    54,   195,     4,  3487,\n",
       "           5,   677,  5910,  1585,   719,  5831,    19,   164,     2,\n",
       "           7,  2171,    10,   294,   102,     4,   641])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert text to sequence of tokens and pad them to ensure euqal length vectors\n",
    "\n",
    "train_seq = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "train_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'i': 3,\n",
       " 'a': 4,\n",
       " 'to': 5,\n",
       " 'of': 6,\n",
       " 'it': 7,\n",
       " 'this': 8,\n",
       " 'is': 9,\n",
       " 'in': 10,\n",
       " 'for': 11,\n",
       " 'that': 12,\n",
       " 'was': 13,\n",
       " 'book': 14,\n",
       " 'you': 15,\n",
       " 'not': 16,\n",
       " 'but': 17,\n",
       " 'with': 18,\n",
       " 'on': 19,\n",
       " 'my': 20,\n",
       " 'have': 21,\n",
       " 'as': 22,\n",
       " 'are': 23,\n",
       " 'one': 24,\n",
       " 'be': 25,\n",
       " 'so': 26,\n",
       " 'all': 27,\n",
       " 'if': 28,\n",
       " 'very': 29,\n",
       " 'like': 30,\n",
       " 'read': 31,\n",
       " 'good': 32,\n",
       " 'great': 33,\n",
       " 'at': 34,\n",
       " 'movie': 35,\n",
       " 'they': 36,\n",
       " 'just': 37,\n",
       " 'about': 38,\n",
       " 'from': 39,\n",
       " 'or': 40,\n",
       " 'would': 41,\n",
       " 'an': 42,\n",
       " 'me': 43,\n",
       " 'out': 44,\n",
       " 'what': 45,\n",
       " 'has': 46,\n",
       " 'more': 47,\n",
       " 'by': 48,\n",
       " 'time': 49,\n",
       " 'had': 50,\n",
       " 'when': 51,\n",
       " 'get': 52,\n",
       " 'will': 53,\n",
       " \"it's\": 54,\n",
       " 'up': 55,\n",
       " 'there': 56,\n",
       " 'no': 57,\n",
       " 'only': 58,\n",
       " 'your': 59,\n",
       " 'can': 60,\n",
       " \"don't\": 61,\n",
       " 'his': 62,\n",
       " 'really': 63,\n",
       " 'who': 64,\n",
       " 'some': 65,\n",
       " 'he': 66,\n",
       " 'well': 67,\n",
       " 'first': 68,\n",
       " 'her': 69,\n",
       " 'much': 70,\n",
       " 'than': 71,\n",
       " 'even': 72,\n",
       " 'do': 73,\n",
       " 'story': 74,\n",
       " 'because': 75,\n",
       " 'them': 76,\n",
       " 'other': 77,\n",
       " 'after': 78,\n",
       " 'buy': 79,\n",
       " 'we': 80,\n",
       " 'were': 81,\n",
       " 'too': 82,\n",
       " 'which': 83,\n",
       " 'she': 84,\n",
       " 'how': 85,\n",
       " 'love': 86,\n",
       " 'these': 87,\n",
       " 'been': 88,\n",
       " 'better': 89,\n",
       " 'best': 90,\n",
       " 'could': 91,\n",
       " 'any': 92,\n",
       " 'into': 93,\n",
       " 'their': 94,\n",
       " 'did': 95,\n",
       " 'books': 96,\n",
       " 'am': 97,\n",
       " 'also': 98,\n",
       " 'work': 99,\n",
       " 'product': 100,\n",
       " 'think': 101,\n",
       " 'then': 102,\n",
       " 'way': 103,\n",
       " \"i'm\": 104,\n",
       " 'its': 105,\n",
       " 'most': 106,\n",
       " 'ever': 107,\n",
       " 'make': 108,\n",
       " 'little': 109,\n",
       " 'many': 110,\n",
       " 'bad': 111,\n",
       " 'over': 112,\n",
       " 'see': 113,\n",
       " 'cd': 114,\n",
       " 'money': 115,\n",
       " 'now': 116,\n",
       " 'never': 117,\n",
       " 'new': 118,\n",
       " 'people': 119,\n",
       " 'does': 120,\n",
       " 'back': 121,\n",
       " 'film': 122,\n",
       " 'music': 123,\n",
       " 'reading': 124,\n",
       " 'know': 125,\n",
       " 'should': 126,\n",
       " 'bought': 127,\n",
       " 'got': 128,\n",
       " 'use': 129,\n",
       " '2': 130,\n",
       " 'made': 131,\n",
       " 'want': 132,\n",
       " 'still': 133,\n",
       " 'off': 134,\n",
       " 'find': 135,\n",
       " 'recommend': 136,\n",
       " \"didn't\": 137,\n",
       " 'two': 138,\n",
       " 'album': 139,\n",
       " 'life': 140,\n",
       " 'game': 141,\n",
       " 'years': 142,\n",
       " 'dvd': 143,\n",
       " 'found': 144,\n",
       " 'say': 145,\n",
       " 'go': 146,\n",
       " \"i've\": 147,\n",
       " 'old': 148,\n",
       " 'again': 149,\n",
       " 'thought': 150,\n",
       " 'through': 151,\n",
       " 'same': 152,\n",
       " 'every': 153,\n",
       " 'while': 154,\n",
       " 'quality': 155,\n",
       " 'thing': 156,\n",
       " 'another': 157,\n",
       " \"can't\": 158,\n",
       " 'characters': 159,\n",
       " 'before': 160,\n",
       " 'down': 161,\n",
       " 'worth': 162,\n",
       " 'put': 163,\n",
       " 'something': 164,\n",
       " 'must': 165,\n",
       " 'why': 166,\n",
       " \"doesn't\": 167,\n",
       " 'written': 168,\n",
       " 'few': 169,\n",
       " 'long': 170,\n",
       " 'being': 171,\n",
       " 'version': 172,\n",
       " 'hard': 173,\n",
       " 'those': 174,\n",
       " 'give': 175,\n",
       " 'our': 176,\n",
       " 'where': 177,\n",
       " '1': 178,\n",
       " 'lot': 179,\n",
       " 'used': 180,\n",
       " 'going': 181,\n",
       " 'anyone': 182,\n",
       " 'makes': 183,\n",
       " 'waste': 184,\n",
       " 'nothing': 185,\n",
       " 'however': 186,\n",
       " '3': 187,\n",
       " 'world': 188,\n",
       " 'watch': 189,\n",
       " 'amazon': 190,\n",
       " 'looking': 191,\n",
       " 'far': 192,\n",
       " 'times': 193,\n",
       " 'here': 194,\n",
       " 'such': 195,\n",
       " 'fun': 196,\n",
       " 'look': 197,\n",
       " '5': 198,\n",
       " 'excellent': 199,\n",
       " 'need': 200,\n",
       " 'boring': 201,\n",
       " 'plot': 202,\n",
       " 'series': 203,\n",
       " 'year': 204,\n",
       " 'enough': 205,\n",
       " 'real': 206,\n",
       " 'easy': 207,\n",
       " 'end': 208,\n",
       " 'sound': 209,\n",
       " 'big': 210,\n",
       " 'interesting': 211,\n",
       " 'novel': 212,\n",
       " 'movies': 213,\n",
       " 'songs': 214,\n",
       " 'without': 215,\n",
       " 'price': 216,\n",
       " 'though': 217,\n",
       " 'own': 218,\n",
       " 'feel': 219,\n",
       " 'take': 220,\n",
       " 'day': 221,\n",
       " 'video': 222,\n",
       " 'disappointed': 223,\n",
       " 'since': 224,\n",
       " 'works': 225,\n",
       " 'original': 226,\n",
       " 'different': 227,\n",
       " 'right': 228,\n",
       " 'set': 229,\n",
       " 'last': 230,\n",
       " 'author': 231,\n",
       " 'things': 232,\n",
       " 'worst': 233,\n",
       " 'actually': 234,\n",
       " 'may': 235,\n",
       " 'around': 236,\n",
       " 'fan': 237,\n",
       " 'pretty': 238,\n",
       " 'classic': 239,\n",
       " 'nice': 240,\n",
       " '4': 241,\n",
       " 'him': 242,\n",
       " 'reviews': 243,\n",
       " 'sure': 244,\n",
       " 'us': 245,\n",
       " 'character': 246,\n",
       " 'stars': 247,\n",
       " 'seen': 248,\n",
       " 'part': 249,\n",
       " 'play': 250,\n",
       " 'come': 251,\n",
       " 'both': 252,\n",
       " 'away': 253,\n",
       " 'keep': 254,\n",
       " 'always': 255,\n",
       " 'said': 256,\n",
       " 'song': 257,\n",
       " 'high': 258,\n",
       " 'wonderful': 259,\n",
       " 'review': 260,\n",
       " 'enjoy': 261,\n",
       " 'seems': 262,\n",
       " 'each': 263,\n",
       " 'bit': 264,\n",
       " 'understand': 265,\n",
       " 'second': 266,\n",
       " 'try': 267,\n",
       " 'item': 268,\n",
       " 'problem': 269,\n",
       " 'writing': 270,\n",
       " 'information': 271,\n",
       " 'yet': 272,\n",
       " 'purchased': 273,\n",
       " 'maybe': 274,\n",
       " 'came': 275,\n",
       " 'anything': 276,\n",
       " 'believe': 277,\n",
       " 'loved': 278,\n",
       " 'almost': 279,\n",
       " 'fact': 280,\n",
       " 'show': 281,\n",
       " 'poor': 282,\n",
       " 'everything': 283,\n",
       " 'small': 284,\n",
       " 'once': 285,\n",
       " 'man': 286,\n",
       " 'quite': 287,\n",
       " 'tried': 288,\n",
       " 'action': 289,\n",
       " 'highly': 290,\n",
       " 'instead': 291,\n",
       " 'whole': 292,\n",
       " 'next': 293,\n",
       " 'less': 294,\n",
       " 'point': 295,\n",
       " 'special': 296,\n",
       " 'three': 297,\n",
       " \"couldn't\": 298,\n",
       " 'family': 299,\n",
       " 'getting': 300,\n",
       " 'using': 301,\n",
       " 'probably': 302,\n",
       " 'trying': 303,\n",
       " 'least': 304,\n",
       " 'true': 305,\n",
       " '10': 306,\n",
       " 'help': 307,\n",
       " 'done': 308,\n",
       " 'star': 309,\n",
       " 'gets': 310,\n",
       " 'full': 311,\n",
       " 'watching': 312,\n",
       " 'having': 313,\n",
       " 'able': 314,\n",
       " 'favorite': 315,\n",
       " 'fine': 316,\n",
       " 'size': 317,\n",
       " 'style': 318,\n",
       " 'live': 319,\n",
       " \"isn't\": 320,\n",
       " 'enjoyed': 321,\n",
       " 'ago': 322,\n",
       " 'especially': 323,\n",
       " 'job': 324,\n",
       " 'minutes': 325,\n",
       " 'amazing': 326,\n",
       " 'together': 327,\n",
       " 'wrong': 328,\n",
       " 'stories': 329,\n",
       " 'definitely': 330,\n",
       " 'went': 331,\n",
       " \"you're\": 332,\n",
       " 'let': 333,\n",
       " \"that's\": 334,\n",
       " 'might': 335,\n",
       " 'school': 336,\n",
       " 'pages': 337,\n",
       " 'perfect': 338,\n",
       " \"wasn't\": 339,\n",
       " 'ordered': 340,\n",
       " 'someone': 341,\n",
       " 'top': 342,\n",
       " 'toy': 343,\n",
       " 'beautiful': 344,\n",
       " 'although': 345,\n",
       " 'short': 346,\n",
       " 'cover': 347,\n",
       " 'liked': 348,\n",
       " 'terrible': 349,\n",
       " \"won't\": 350,\n",
       " 'else': 351,\n",
       " 'fit': 352,\n",
       " 'horrible': 353,\n",
       " 'piece': 354,\n",
       " 'place': 355,\n",
       " 'please': 356,\n",
       " 'buying': 357,\n",
       " 'several': 358,\n",
       " 'wish': 359,\n",
       " 'rather': 360,\n",
       " 'card': 361,\n",
       " 'kind': 362,\n",
       " 'wanted': 363,\n",
       " 'half': 364,\n",
       " 'heard': 365,\n",
       " 'already': 366,\n",
       " 'listen': 367,\n",
       " 'reason': 368,\n",
       " 'received': 369,\n",
       " 'until': 370,\n",
       " 'comes': 371,\n",
       " 'acting': 372,\n",
       " 'history': 373,\n",
       " 'months': 374,\n",
       " 'ok': 375,\n",
       " 'everyone': 376,\n",
       " 'mind': 377,\n",
       " 'tell': 378,\n",
       " 'class': 379,\n",
       " 'page': 380,\n",
       " 'computer': 381,\n",
       " 'between': 382,\n",
       " 'others': 383,\n",
       " 'days': 384,\n",
       " 'took': 385,\n",
       " 'purchase': 386,\n",
       " 'picture': 387,\n",
       " 'power': 388,\n",
       " 'reader': 389,\n",
       " 'gave': 390,\n",
       " 'christmas': 391,\n",
       " 'camera': 392,\n",
       " 'line': 393,\n",
       " 'order': 394,\n",
       " 'house': 395,\n",
       " 'start': 396,\n",
       " 'kids': 397,\n",
       " 'bed': 398,\n",
       " 'completely': 399,\n",
       " 'takes': 400,\n",
       " 'funny': 401,\n",
       " 'making': 402,\n",
       " 'simply': 403,\n",
       " 'saw': 404,\n",
       " 'night': 405,\n",
       " 'fast': 406,\n",
       " 'effects': 407,\n",
       " 'box': 408,\n",
       " 'either': 409,\n",
       " 'children': 410,\n",
       " 'difficult': 411,\n",
       " 'return': 412,\n",
       " 'happy': 413,\n",
       " 'collection': 414,\n",
       " 'idea': 415,\n",
       " 'light': 416,\n",
       " 'looks': 417,\n",
       " 'hours': 418,\n",
       " 'english': 419,\n",
       " 'overall': 420,\n",
       " 'cheap': 421,\n",
       " \"i'd\": 422,\n",
       " 'seem': 423,\n",
       " 'young': 424,\n",
       " 'needed': 425,\n",
       " 'myself': 426,\n",
       " 'son': 427,\n",
       " 'cannot': 428,\n",
       " 'future': 429,\n",
       " 'player': 430,\n",
       " \"you'll\": 431,\n",
       " 'couple': 432,\n",
       " 'air': 433,\n",
       " 'words': 434,\n",
       " 'felt': 435,\n",
       " 'absolutely': 436,\n",
       " 'save': 437,\n",
       " 'battery': 438,\n",
       " 'rest': 439,\n",
       " 'problems': 440,\n",
       " 'edition': 441,\n",
       " 'person': 442,\n",
       " 'home': 443,\n",
       " 'left': 444,\n",
       " 'started': 445,\n",
       " 'band': 446,\n",
       " 'lost': 447,\n",
       " 'worked': 448,\n",
       " 'truly': 449,\n",
       " 'wait': 450,\n",
       " 'ray': 451,\n",
       " 'side': 452,\n",
       " 'scenes': 453,\n",
       " 'working': 454,\n",
       " 'etc': 455,\n",
       " 'thinking': 456,\n",
       " 'simple': 457,\n",
       " 'women': 458,\n",
       " 'goes': 459,\n",
       " 'sense': 460,\n",
       " 'stuff': 461,\n",
       " 'stay': 462,\n",
       " 'fiction': 463,\n",
       " 'woman': 464,\n",
       " 'itself': 465,\n",
       " 'free': 466,\n",
       " 'title': 467,\n",
       " \"wouldn't\": 468,\n",
       " 'experience': 469,\n",
       " 'guess': 470,\n",
       " 'hear': 471,\n",
       " 'entire': 472,\n",
       " 'awesome': 473,\n",
       " 'track': 474,\n",
       " 'friends': 475,\n",
       " 'missing': 476,\n",
       " 'playing': 477,\n",
       " 'american': 478,\n",
       " 'course': 479,\n",
       " 'ending': 480,\n",
       " 'cool': 481,\n",
       " 'company': 482,\n",
       " 'case': 483,\n",
       " 'gives': 484,\n",
       " 'daughter': 485,\n",
       " 'under': 486,\n",
       " 'slow': 487,\n",
       " 'shows': 488,\n",
       " 'hope': 489,\n",
       " 'support': 490,\n",
       " 'extremely': 491,\n",
       " 'copy': 492,\n",
       " 'dont': 493,\n",
       " 'type': 494,\n",
       " 'based': 495,\n",
       " \"i'll\": 496,\n",
       " 'parts': 497,\n",
       " 'language': 498,\n",
       " 'horror': 499,\n",
       " 'complete': 500,\n",
       " 'main': 501,\n",
       " 'pictures': 502,\n",
       " 'yourself': 503,\n",
       " 'finally': 504,\n",
       " 'expected': 505,\n",
       " 'girl': 506,\n",
       " 'glad': 507,\n",
       " '6': 508,\n",
       " 'played': 509,\n",
       " 'past': 510,\n",
       " 'learn': 511,\n",
       " 'tracks': 512,\n",
       " 'self': 513,\n",
       " 'unfortunately': 514,\n",
       " 'along': 515,\n",
       " 'hold': 516,\n",
       " 'run': 517,\n",
       " 'comfortable': 518,\n",
       " 'ones': 519,\n",
       " 'care': 520,\n",
       " 'garbage': 521,\n",
       " 'says': 522,\n",
       " 'needs': 523,\n",
       " 'dark': 524,\n",
       " 'sounds': 525,\n",
       " 'name': 526,\n",
       " 'scary': 527,\n",
       " 'yes': 528,\n",
       " 'today': 529,\n",
       " 'unless': 530,\n",
       " 'easily': 531,\n",
       " 'during': 532,\n",
       " 'release': 533,\n",
       " 'totally': 534,\n",
       " 'huge': 535,\n",
       " 'entertaining': 536,\n",
       " 'available': 537,\n",
       " 'later': 538,\n",
       " 'doing': 539,\n",
       " 'view': 540,\n",
       " 'mr': 541,\n",
       " 'called': 542,\n",
       " '1984': 543,\n",
       " 'kept': 544,\n",
       " 'tv': 545,\n",
       " 'pay': 546,\n",
       " 'loves': 547,\n",
       " 'society': 548,\n",
       " 'remember': 549,\n",
       " 'black': 550,\n",
       " 'guy': 551,\n",
       " 'told': 552,\n",
       " 'gift': 553,\n",
       " \"there's\": 554,\n",
       " 'single': 555,\n",
       " 'heart': 556,\n",
       " 'given': 557,\n",
       " 'seemed': 558,\n",
       " 'five': 559,\n",
       " 'sometimes': 560,\n",
       " 'recommended': 561,\n",
       " 'war': 562,\n",
       " 'service': 563,\n",
       " 'write': 564,\n",
       " 'friend': 565,\n",
       " 'store': 566,\n",
       " 'turn': 567,\n",
       " 'opinion': 568,\n",
       " 'child': 569,\n",
       " 'color': 570,\n",
       " 'word': 571,\n",
       " 'expect': 572,\n",
       " 'sorry': 573,\n",
       " 'early': 574,\n",
       " 'disappointing': 575,\n",
       " 'within': 576,\n",
       " 'weeks': 577,\n",
       " 'description': 578,\n",
       " 'interested': 579,\n",
       " 'become': 580,\n",
       " 'deal': 581,\n",
       " 'text': 582,\n",
       " 'stop': 583,\n",
       " 'fans': 584,\n",
       " 'cut': 585,\n",
       " 'clear': 586,\n",
       " 'age': 587,\n",
       " 'decided': 588,\n",
       " 'often': 589,\n",
       " 'rock': 590,\n",
       " 'mean': 591,\n",
       " 'looked': 592,\n",
       " 'stupid': 593,\n",
       " 'head': 594,\n",
       " 'worse': 595,\n",
       " 'kindle': 596,\n",
       " 'beginning': 597,\n",
       " 'awful': 598,\n",
       " 'lots': 599,\n",
       " 'low': 600,\n",
       " 'paper': 601,\n",
       " 'perhaps': 602,\n",
       " 'follow': 603,\n",
       " 'human': 604,\n",
       " 'week': 605,\n",
       " 'print': 606,\n",
       " 'change': 607,\n",
       " 'important': 608,\n",
       " 'screen': 609,\n",
       " 'value': 610,\n",
       " 'science': 611,\n",
       " 'non': 612,\n",
       " 'level': 613,\n",
       " 'number': 614,\n",
       " 'ideas': 615,\n",
       " 'fantastic': 616,\n",
       " 'art': 617,\n",
       " 'games': 618,\n",
       " 'god': 619,\n",
       " 'wear': 620,\n",
       " 'modern': 621,\n",
       " 'john': 622,\n",
       " 'watched': 623,\n",
       " 'close': 624,\n",
       " 'strong': 625,\n",
       " 'supposed': 626,\n",
       " 'chapter': 627,\n",
       " 'arrived': 628,\n",
       " 'husband': 629,\n",
       " 'wrote': 630,\n",
       " 'dance': 631,\n",
       " 'listening': 632,\n",
       " 'charger': 633,\n",
       " 'voice': 634,\n",
       " 'u': 635,\n",
       " 'films': 636,\n",
       " 'tale': 637,\n",
       " \"haven't\": 638,\n",
       " 'add': 639,\n",
       " 'four': 640,\n",
       " 'month': 641,\n",
       " 'sad': 642,\n",
       " 'usually': 643,\n",
       " 'replacement': 644,\n",
       " 'hot': 645,\n",
       " 'ear': 646,\n",
       " 'printer': 647,\n",
       " 'quickly': 648,\n",
       " 'disc': 649,\n",
       " 'scene': 650,\n",
       " 'literature': 651,\n",
       " 'taking': 652,\n",
       " 'check': 653,\n",
       " '8': 654,\n",
       " 'cost': 655,\n",
       " 'material': 656,\n",
       " 's': 657,\n",
       " 'poorly': 658,\n",
       " 'disappointment': 659,\n",
       " 'oh': 660,\n",
       " 'large': 661,\n",
       " 'longer': 662,\n",
       " 'sent': 663,\n",
       " 'helpful': 664,\n",
       " 'finish': 665,\n",
       " 'spent': 666,\n",
       " 'soon': 667,\n",
       " 'writer': 668,\n",
       " '20': 669,\n",
       " 'audio': 670,\n",
       " 'room': 671,\n",
       " 'software': 672,\n",
       " 'e': 673,\n",
       " 'blu': 674,\n",
       " 'pieces': 675,\n",
       " 'plastic': 676,\n",
       " 'spend': 677,\n",
       " 'extra': 678,\n",
       " 'lives': 679,\n",
       " 'tape': 680,\n",
       " 'junk': 681,\n",
       " 'shipping': 682,\n",
       " 'forward': 683,\n",
       " 'useful': 684,\n",
       " 'actors': 685,\n",
       " 'fall': 686,\n",
       " 'library': 687,\n",
       " 'unit': 688,\n",
       " 'attention': 689,\n",
       " 'matter': 690,\n",
       " 'example': 691,\n",
       " '30': 692,\n",
       " 'otherwise': 693,\n",
       " 'annoying': 694,\n",
       " 'charge': 695,\n",
       " 'features': 696,\n",
       " 'certainly': 697,\n",
       " 'message': 698,\n",
       " 'albums': 699,\n",
       " 'boots': 700,\n",
       " 'interest': 701,\n",
       " 'hand': 702,\n",
       " 'white': 703,\n",
       " 'except': 704,\n",
       " 'control': 705,\n",
       " 'happened': 706,\n",
       " 'open': 707,\n",
       " 'group': 708,\n",
       " 'broke': 709,\n",
       " 'taken': 710,\n",
       " 'humor': 711,\n",
       " '15': 712,\n",
       " 'water': 713,\n",
       " 'previous': 714,\n",
       " 'apart': 715,\n",
       " 'against': 716,\n",
       " 'anyway': 717,\n",
       " 'pop': 718,\n",
       " 'plus': 719,\n",
       " 'c': 720,\n",
       " 'thank': 721,\n",
       " 'figure': 722,\n",
       " 'performance': 723,\n",
       " 'hate': 724,\n",
       " 'none': 725,\n",
       " '100': 726,\n",
       " 'beyond': 727,\n",
       " 'general': 728,\n",
       " 'dog': 729,\n",
       " 'brother': 730,\n",
       " 'guys': 731,\n",
       " 'actual': 732,\n",
       " 'adapter': 733,\n",
       " 'wonder': 734,\n",
       " 'suggest': 735,\n",
       " 'miss': 736,\n",
       " 'pick': 737,\n",
       " '451': 738,\n",
       " 'inside': 739,\n",
       " 'bottom': 740,\n",
       " 'happen': 741,\n",
       " 'turned': 742,\n",
       " 'system': 743,\n",
       " 'face': 744,\n",
       " 'novels': 745,\n",
       " 'paid': 746,\n",
       " 'agree': 747,\n",
       " 'known': 748,\n",
       " 'call': 749,\n",
       " 'test': 750,\n",
       " 'enjoyable': 751,\n",
       " 're': 752,\n",
       " 'due': 753,\n",
       " 'guide': 754,\n",
       " 'hour': 755,\n",
       " 'possible': 756,\n",
       " 'content': 757,\n",
       " 'exactly': 758,\n",
       " 'break': 759,\n",
       " 'blue': 760,\n",
       " 'returned': 761,\n",
       " 'space': 762,\n",
       " 'country': 763,\n",
       " 'decent': 764,\n",
       " 'creative': 765,\n",
       " 'record': 766,\n",
       " 'graphics': 767,\n",
       " 'readers': 768,\n",
       " 'expensive': 769,\n",
       " 'death': 770,\n",
       " 'wife': 771,\n",
       " '7': 772,\n",
       " \"they're\": 773,\n",
       " 'plug': 774,\n",
       " 'seller': 775,\n",
       " 'hoping': 776,\n",
       " 'sleep': 777,\n",
       " 'coming': 778,\n",
       " 'condition': 779,\n",
       " 'cable': 780,\n",
       " 'personal': 781,\n",
       " 'feeling': 782,\n",
       " 'leave': 783,\n",
       " 'included': 784,\n",
       " 'basic': 785,\n",
       " 'metal': 786,\n",
       " 'apple': 787,\n",
       " 'throughout': 788,\n",
       " 'clearly': 789,\n",
       " 'bother': 790,\n",
       " 'thanks': 791,\n",
       " 'brand': 792,\n",
       " 'greatest': 793,\n",
       " 'com': 794,\n",
       " 'stopped': 795,\n",
       " 'total': 796,\n",
       " 'sex': 797,\n",
       " 'effort': 798,\n",
       " 'crap': 799,\n",
       " 'quick': 800,\n",
       " 'season': 801,\n",
       " 'detail': 802,\n",
       " 'released': 803,\n",
       " 'giving': 804,\n",
       " 'wants': 805,\n",
       " 'confusing': 806,\n",
       " 'mine': 807,\n",
       " 'issues': 808,\n",
       " 'stand': 809,\n",
       " 'design': 810,\n",
       " 'beat': 811,\n",
       " 'production': 812,\n",
       " 'chance': 813,\n",
       " 'fire': 814,\n",
       " 'skin': 815,\n",
       " 'move': 816,\n",
       " 'hit': 817,\n",
       " 'products': 818,\n",
       " 'concert': 819,\n",
       " 'canon': 820,\n",
       " 'knew': 821,\n",
       " 'dead': 822,\n",
       " 'eyes': 823,\n",
       " 'lack': 824,\n",
       " 'sony': 825,\n",
       " 'major': 826,\n",
       " 'uses': 827,\n",
       " '9': 828,\n",
       " 'la': 829,\n",
       " 'somewhat': 830,\n",
       " 'foundation': 831,\n",
       " 'fell': 832,\n",
       " 'fascinating': 833,\n",
       " 'seeing': 834,\n",
       " 'middle': 835,\n",
       " 'third': 836,\n",
       " 'rice': 837,\n",
       " 'cave': 838,\n",
       " 'learned': 839,\n",
       " 'men': 840,\n",
       " 'surprised': 841,\n",
       " 'romance': 842,\n",
       " 'boy': 843,\n",
       " 'addition': 844,\n",
       " 'dull': 845,\n",
       " 'subject': 846,\n",
       " 'thin': 847,\n",
       " 'plain': 848,\n",
       " 'baby': 849,\n",
       " 'waiting': 850,\n",
       " 'starts': 851,\n",
       " 'hp': 852,\n",
       " 'twice': 853,\n",
       " 'talking': 854,\n",
       " 'older': 855,\n",
       " 'excited': 856,\n",
       " \"he's\": 857,\n",
       " 'talk': 858,\n",
       " 'despite': 859,\n",
       " 'note': 860,\n",
       " 'jack': 861,\n",
       " 'sort': 862,\n",
       " 'wow': 863,\n",
       " 'manson': 864,\n",
       " 'beware': 865,\n",
       " 'behind': 866,\n",
       " 'b': 867,\n",
       " 'running': 868,\n",
       " 'ended': 869,\n",
       " 'similar': 870,\n",
       " 'basically': 871,\n",
       " 'body': 872,\n",
       " 'broken': 873,\n",
       " 'orwell': 874,\n",
       " 'forget': 875,\n",
       " 'send': 876,\n",
       " 'pair': 877,\n",
       " 'memory': 878,\n",
       " 'incredible': 879,\n",
       " 'historical': 880,\n",
       " '0': 881,\n",
       " '50': 882,\n",
       " 'serious': 883,\n",
       " 'avoid': 884,\n",
       " 'de': 885,\n",
       " 'warning': 886,\n",
       " 'including': 887,\n",
       " 'amount': 888,\n",
       " 'genre': 889,\n",
       " 'parents': 890,\n",
       " 'knowledge': 891,\n",
       " 'bradbury': 892,\n",
       " 'soundtrack': 893,\n",
       " 'brilliant': 894,\n",
       " 'useless': 895,\n",
       " 'max': 896,\n",
       " 'list': 897,\n",
       " 'boot': 898,\n",
       " 'cast': 899,\n",
       " 'super': 900,\n",
       " 'rating': 901,\n",
       " 'customer': 902,\n",
       " 'truth': 903,\n",
       " 'stick': 904,\n",
       " 'fahrenheit': 905,\n",
       " 'america': 906,\n",
       " 'late': 907,\n",
       " 'reference': 908,\n",
       " 'moving': 909,\n",
       " 'skip': 910,\n",
       " 'trip': 911,\n",
       " 'living': 912,\n",
       " 'government': 913,\n",
       " 'okay': 914,\n",
       " 'hands': 915,\n",
       " 'finished': 916,\n",
       " 'alot': 917,\n",
       " 'car': 918,\n",
       " 'lyrics': 919,\n",
       " 'weight': 920,\n",
       " 'date': 921,\n",
       " 'radio': 922,\n",
       " 'students': 923,\n",
       " 'fi': 924,\n",
       " 'themselves': 925,\n",
       " 'waist': 926,\n",
       " 'stuck': 927,\n",
       " 'transformers': 928,\n",
       " 'ms': 929,\n",
       " 'digital': 930,\n",
       " 'insight': 931,\n",
       " 'pass': 932,\n",
       " '12': 933,\n",
       " 'impressed': 934,\n",
       " 'trash': 935,\n",
       " 'heavy': 936,\n",
       " 'changed': 937,\n",
       " 'weak': 938,\n",
       " 'events': 939,\n",
       " 'seriously': 940,\n",
       " 'replace': 941,\n",
       " 'clean': 942,\n",
       " 'shame': 943,\n",
       " 'authors': 944,\n",
       " 'questions': 945,\n",
       " 'correct': 946,\n",
       " 'mother': 947,\n",
       " 'study': 948,\n",
       " 'happens': 949,\n",
       " 'pleased': 950,\n",
       " 'keeps': 951,\n",
       " 'mix': 952,\n",
       " 'model': 953,\n",
       " 'form': 954,\n",
       " 'himself': 955,\n",
       " 'adventure': 956,\n",
       " 'informative': 957,\n",
       " 'shot': 958,\n",
       " 'recently': 959,\n",
       " 'haunting': 960,\n",
       " 'windows': 961,\n",
       " 'century': 962,\n",
       " 'alone': 963,\n",
       " 'bored': 964,\n",
       " 'exciting': 965,\n",
       " 'cute': 966,\n",
       " 'cause': 967,\n",
       " 'development': 968,\n",
       " 'details': 969,\n",
       " 'putting': 970,\n",
       " 'crazy': 971,\n",
       " 'masterpiece': 972,\n",
       " 'imagine': 973,\n",
       " 'grade': 974,\n",
       " 'straight': 975,\n",
       " 'ridiculous': 976,\n",
       " 'background': 977,\n",
       " 'flat': 978,\n",
       " 'across': 979,\n",
       " 'research': 980,\n",
       " 'issue': 981,\n",
       " 'appears': 982,\n",
       " 'sci': 983,\n",
       " 'learning': 984,\n",
       " 'advice': 985,\n",
       " 'front': 986,\n",
       " 'wasted': 987,\n",
       " 'forever': 988,\n",
       " 'mattress': 989,\n",
       " 'means': 990,\n",
       " 'turns': 991,\n",
       " 'comedy': 992,\n",
       " 'difference': 993,\n",
       " 'nearly': 994,\n",
       " 'reviewers': 995,\n",
       " 'incredibly': 996,\n",
       " 'recording': 997,\n",
       " 'obviously': 998,\n",
       " 'business': 999,\n",
       " 'introduction': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = token.word_index\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network CNN\n",
    "def create_cnn():\n",
    "    # add an Input layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "    \n",
    "    #add the word embedding layer\n",
    "    \n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300,\n",
    "                                       weights=[embedding_matrix], \n",
    "                                       trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "    \n",
    "    # add the convolutional layer\n",
    "    \n",
    "    conv_layer = layer.Convolution1D(100, 3, activation='relu')(embdding_layer)\n",
    "    \n",
    "    # add the pooling layers\n",
    "    \n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "    \n",
    "    \n",
    "    #add the out layers\n",
    "    output_layer1 = layers.Dense(50, activation='relu')(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(out_put_layer1)\n",
    "    \n",
    "    output_layer2 = layers.Dense(1, activation='sigmoid')(output_layer1)\n",
    "    \n",
    "    \n",
    "    # compile the model\n",
    "    model = models.Model(inuts=input_layer, outputs=output_layer2)\n",
    "    \n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_cnn()\n",
    "\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x,\n",
    "                       is_neural_net=True)\n",
    "\n",
    "print(f\"CNN, Word embeddings: { accuracy}\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  RNN - LSTM\n",
    "def create_rnn_lstm():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_lstm()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print(\"RNN-LSTM, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Neural Network GRU\n",
    "def create_rnn_gru():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the GRU Layer\n",
    "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_gru()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print(\"RNN-GRU, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional RNN\n",
    "def create_bidirectional_rnn():\n",
    "        # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_bidirectional_rnn()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print(\"RNN-Bidirectional, Word Embeddings\",  accuracy)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Convolutional Neural Network\n",
    "\n",
    "def create_rcnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "    \n",
    "    # Add the recurrent layer\n",
    "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
    "    \n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rcnn()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print(\"CNN, Word Embeddings\",  accurac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
